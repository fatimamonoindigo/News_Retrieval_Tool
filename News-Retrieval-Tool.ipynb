{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5707ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the news keyword you want to search: Chandrayaan ISRO\n",
      "Skipping invalid URL: https://www.nytimes.com/live/2023/08/23/science/india-moon-landing-chandrayaan-3\n",
      "Source: https://www.independent.co.uk/space/chandrayaan-3-isro-live-moon-landing-time-b2397874.html\n",
      "Title: chandrayaan live indian space agency achieves historic moon mission landing\n",
      "Summary: chandrayaan 3 update: Indian moon landing\n",
      "------\n",
      "Relevance: 1.00\n",
      "Accuracy: 0.82\n",
      "------\n",
      "Source: https://www.independent.co.uk/space/chandrayaan-3-isro-live-moon-landing-time-b2397874.html\n",
      "Title: chandrayaan live indian space agency achieves historic moon mission landing\n",
      "Summary: chandrayaan 3 update: Indian moon landing\n",
      "------\n",
      "Relevance: 1.00\n",
      "Accuracy: 0.82\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the NLTK resources (run this once)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "PURPLE = \"\\033[95m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "def display_colored_link(link, is_reputed):\n",
    "    color = PURPLE if is_reputed else \"\"\n",
    "    reset_color = RESET if is_reputed else \"\"\n",
    "    print(f\"{color}{link}{reset_color}\")\n",
    "\n",
    "# List of trusted news sources\n",
    "trusted_news_sources = [\n",
    "    \"bbc.co.uk\",\n",
    "    \"cnn.com\",\n",
    "    \"nytimes.com\",\n",
    "    \"washingtonpost.com\",\n",
    "    \"guardian.com\",\n",
    "    \"reuters.com\",\n",
    "    \"aljazeera.com\",\n",
    "    \"apnews.com\",\n",
    "    \"wsj.com\",\n",
    "    \"bbc.com/news/world\",\n",
    "    \"timesofindia.indiatimes.com/\",\n",
    "    \"m.timesofindia.com\",\n",
    "    \"uk.news.yahoo.com\",\n",
    "    \"https://pressgazette.co.uk\",\n",
    "    \"https://abcnews.go.com/\",\n",
    "    \"independent.co.uk\",\n",
    "    \n",
    "    \n",
    "    # Add more trusted news sources here\n",
    "]\n",
    "\n",
    "def is_reputed_news_source(url):\n",
    "    return any(source in url for source in trusted_news_sources)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    cleaned_text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    # Lowercase the text\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def is_url_valid(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        return response.status_code in range(200, 300)  # Check if the status code is in the success range\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "def get_related_news(search_keyword):\n",
    "    # Perform Google search\n",
    "    query = f\"{search_keyword} news\"\n",
    "    search_results = list(search(query, num_results=5, lang=\"en\"))\n",
    "\n",
    "    related_news = []\n",
    "\n",
    "    # Retrieve and scrape news articles\n",
    "    for url in search_results:\n",
    "        if is_url_valid(url):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Extract the Google snippet for the news article\n",
    "                snippet = soup.find('meta', attrs={'name': 'description'})\n",
    "                if snippet:\n",
    "                    snippet = snippet['content']\n",
    "\n",
    "                    # Extract the news article content\n",
    "                    article = soup.find('article')  # Adjust based on the website's HTML structure\n",
    "                    if article:\n",
    "                        news_title_element = article.find('h1')\n",
    "                        news_content_elements = article.find_all('p')\n",
    "\n",
    "                        if news_title_element and news_content_elements:\n",
    "                            news_title = news_title_element.text.strip()\n",
    "                            news_content = '\\n'.join([p.text.strip() for p in news_content_elements])\n",
    "\n",
    "                            # Clean the title and content\n",
    "                            cleaned_title = clean_text(news_title)\n",
    "                            cleaned_content = clean_text(news_content)\n",
    "\n",
    "                            # Tokenize the content and remove stopwords\n",
    "                            stop_words = set(stopwords.words('english'))\n",
    "                            title_tokens = word_tokenize(cleaned_title)\n",
    "                            content_tokens = word_tokenize(cleaned_content)\n",
    "\n",
    "                            title_tokens = [token for token in title_tokens if token not in stop_words]\n",
    "                            content_tokens = [token for token in content_tokens if token not in stop_words]\n",
    "\n",
    "                            # Check if the URL is from a reputed news source\n",
    "                            is_reputed = is_reputed_news_source(url)\n",
    "\n",
    "                            related_news.append({'source': url, 'title': title_tokens, 'content': content_tokens, 'snippet': snippet, 'is_reputed': is_reputed})\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error retrieving article from {url}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Skipping invalid URL: {url}\")\n",
    "\n",
    "    return related_news\n",
    "\n",
    "# Example usage\n",
    "def calculate_relevance(claim, article_content):\n",
    "    claim_tokens = set(claim)\n",
    "    content_tokens = set(article_content)\n",
    "    common_tokens = claim_tokens.intersection(content_tokens)\n",
    "    relevance = len(common_tokens) / len(claim_tokens)\n",
    "    if not claim_tokens:\n",
    "        return 0.0\n",
    "    return relevance\n",
    "\n",
    "# Function to calculate accuracy of summarized content\n",
    "def calculate_accuracy(original_text, summarized_text):\n",
    "    original_tokens = set(original_text)\n",
    "    summarized_tokens = set(summarized_text)\n",
    "    common_tokens = original_tokens.intersection(summarized_tokens)\n",
    "    accuracy = len(common_tokens) / len(original_tokens)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "search_keyword = input(\"Enter the news keyword you want to search: \")\n",
    "\n",
    "related_news = get_related_news(search_keyword)\n",
    "\n",
    "# Display the related news articles\n",
    "for article in related_news:\n",
    "    source_prefix = \"TRUSTED: \" if article['is_reputed'] else \"Source: \"\n",
    "    print(f\"{source_prefix}\", end=\"\")\n",
    "    display_colored_link(article['source'], article['is_reputed'])\n",
    "    print(f\"Title: {' '.join(article['title'])}\")\n",
    "    #print(f\"Content: {' '.join(article['content'])}\")\n",
    "    print(f\"Summary: {article['snippet']}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Calculate relevance and accuracy\n",
    "    claim_tokens = word_tokenize(clean_text(search_keyword))\n",
    "    content_tokens = word_tokenize(clean_text(' '.join(article['content'])))\n",
    "    title_tokens = word_tokenize(clean_text(' '.join(article['title'])))\n",
    "    snippet_tokens = word_tokenize(clean_text(article['snippet']))\n",
    "    \n",
    "    relevance = calculate_relevance(claim_tokens, content_tokens)\n",
    "    accuracy = calculate_accuracy(title_tokens + snippet_tokens, content_tokens)\n",
    "\n",
    "    print(f\"Relevance: {relevance:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c6da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import messagebox\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"News Scrapper\")\n",
    "\n",
    "# Create a Text widget for user input\n",
    "search_keyword_entry = tk.Entry(window, width=50)\n",
    "search_keyword_entry.pack(pady=10)\n",
    "\n",
    "# Create a ScrolledText widget to display the results\n",
    "result_text = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=80, height=20)\n",
    "result_text.pack(pady=10)\n",
    "\n",
    "def run_news_scrapper():\n",
    "    # Redirect standard output to capture the print statements\n",
    "    output_buffer = StringIO()\n",
    "    sys.stdout = output_buffer\n",
    "\n",
    "    # Get the search keyword from the user input\n",
    "    search_keyword = search_keyword_entry.get().strip()\n",
    "\n",
    "    # Get related news articles\n",
    "    related_news = get_related_news(search_keyword)\n",
    "\n",
    "    # Prepare the result to be displayed in the ScrolledText widget\n",
    "    result = \"\"\n",
    "    for article in related_news:\n",
    "        source_prefix = \"TRUSTED: \" if article['is_reputed'] else \"Source: \"\n",
    "        result += f\"{source_prefix}{article['source']}\\n\"\n",
    "        result += f\"Title: {' '.join(article['title'])}\\n\"\n",
    "        #result += f\"Content: {' '.join(article['content'])}\\n\"\n",
    "        result += f\"Summary: {article['snippet']}\\n\"\n",
    "        result += \"------\\n\"\n",
    "\n",
    "    # Display the result in the ScrolledText widget\n",
    "    result_text.delete(1.0, tk.END)\n",
    "    result_text.insert(tk.END, result)\n",
    "\n",
    "    # Restore standard output\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "def on_exit():\n",
    "    if messagebox.askyesno(\"Exit\", \"Do you want to exit the application?\"):\n",
    "        window.destroy()\n",
    "\n",
    "# Create a button to run the news scrapper\n",
    "run_button = tk.Button(window, text=\"Run News Scrapper\", command=run_news_scrapper)\n",
    "run_button.pack(pady=10)\n",
    "\n",
    "# Create an Exit button\n",
    "exit_button = tk.Button(window, text=\"Exit\", command=on_exit)\n",
    "exit_button.pack(pady=5)\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Ethical issue\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Set the user-agent header to identify your tool\n",
    "HEADERS = {\n",
    "    'User-Agent': 'NewsRetrievalTool/1.0)'\n",
    "}\n",
    "\n",
    "def is_allowed_by_robots(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = urljoin(parsed_url.geturl(), \"/robots.txt\")\n",
    "    try:\n",
    "        response = requests.get(robots_url, headers=HEADERS)\n",
    "        return \"/news\" in response.text  # Check if /news is allowed, adjust as needed\n",
    "    except requests.exceptions.RequestException:\n",
    "        return True  # Assume allowed if unable to fetch robots.txt\n",
    "\n",
    "def is_url_valid(url):\n",
    "    try:\n",
    "        response = requests.head(url, headers=HEADERS)\n",
    "        return response.status_code in range(200, 300) and is_allowed_by_robots(url)\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
